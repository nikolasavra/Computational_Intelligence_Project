{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing characters with <>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_chr = r'<.*?>'\n",
    "\n",
    "infile = open('train-data.dat', 'r')\n",
    "lines = infile.readlines()\n",
    "infile.close()\n",
    "     \n",
    "outfile = open('train-data_new.dat', 'w')\n",
    "\n",
    "for str in lines:\n",
    "    new_str = re.sub(del_chr, '', str)\n",
    "    outfile.writelines(new_str)\n",
    "    \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_chr = r'<.*?>'\n",
    "\n",
    "infile = open('test-data.dat', 'r')\n",
    "lines = infile.readlines()\n",
    "infile.close()\n",
    "     \n",
    "outfile = open('test-data_new.dat', 'w')\n",
    "\n",
    "for str in lines:\n",
    "    new_str = re.sub(del_chr, '', str)\n",
    "    outfile.writelines(new_str)\n",
    "    \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the BoW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count1 = {}\n",
    "\n",
    "dataset = open('train-data_new.dat', 'r')\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count1.keys():\n",
    "            word2count1[word] = 1\n",
    "        else:\n",
    "            word2count1[word] += 1\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2count2 = {}\n",
    "\n",
    "dataset = open('test-data_new.dat', 'r')\n",
    "for data in dataset:\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in words:\n",
    "        if word not in word2count2.keys():\n",
    "            word2count2[word] = 1\n",
    "        else:\n",
    "            word2count2[word] += 1\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2count1), len(word2count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('train-data_new.dat', 'r')\n",
    "lines1 = infile.readlines()\n",
    "infile.close()\n",
    "     \n",
    "#outfile = open('train-data_new_tokenized.dat', 'w')\n",
    "\n",
    "for str in lines1:\n",
    "    new_str = word_tokenize(str)\n",
    "    print(new_str)\n",
    "    #outfile.writelines(new_str)\n",
    "    \n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('test-data_new.dat', 'r')\n",
    "lines2 = infile.readlines()\n",
    "infile.close()\n",
    "     \n",
    "#outfile = open('train-data_new_tokenized.dat', 'w')\n",
    "\n",
    "for str in lines2:\n",
    "    new_str = word_tokenize(str)\n",
    "    print(new_str)\n",
    "    #outfile.writelines(new_str)\n",
    "    \n",
    "#outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Tokenizer()\n",
    "model1.fit_on_texts(lines1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep1 = model1.texts_to_matrix(lines1, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing one extra column\n",
    "rep1 = np.delete(rep1,0,1)\n",
    "rep1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum element to chech for outliers\n",
    "maxElement = np.amax(rep1)\n",
    "maxElement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Tokenizer()\n",
    "model2.fit_on_texts(lines2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep2 = model2.texts_to_matrix(lines2, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing one extra column\n",
    "rep2 = np.delete(rep2,0,1)\n",
    "rep2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxElement = np.amax(rep2)\n",
    "maxElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model1.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model2.word_index.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the vocabularies' lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocabulary = list(model1.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vocabulary = list(model2.word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(test_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find missing words from test vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_match_elements(list_a, list_b):\n",
    "    non_match = []\n",
    "    for i in list_a:\n",
    "        if i not in list_b:\n",
    "            non_match.append(i)\n",
    "    return non_match\n",
    "       \n",
    "\n",
    "list_a = train_vocabulary\n",
    "list_b = test_vocabulary\n",
    "\n",
    "non_match = non_match_elements(list_a, list_b)\n",
    "print(\"No match elements: \", non_match)\n",
    "print(\"Number of no match elements: \", len(non_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a dataframe out of train vocabulary\n",
    "df_train = pd.DataFrame(rep1, columns = train_vocabulary)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting it\n",
    "df_train = df_train.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Making a dataframe out of test vocabulary\n",
    "df_test = pd.DataFrame(rep2, columns = test_vocabulary)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting it\n",
    "df_test = df_test.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a dataframe out of the missing words from test vocabulary\n",
    "df_temp = pd.DataFrame(columns = non_match)\n",
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Combining test vocabulary and missing words to match the size of train vocabulary\n",
    "df_test_final = df_test.append(df_temp)\n",
    "df_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change null values with zeros\n",
    "df_test_final = df_test_final.fillna(0)\n",
    "df_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = df_test_final.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_final = df_test_final.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_final, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"train-label.dat\"\n",
    "colnames = []\n",
    "\n",
    "for i in range(20):\n",
    "    colnames.append(i)\n",
    "\n",
    "df_train_label = pd.read_csv(file,sep=' ', names=colnames)\n",
    "df_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"test-label.dat\"\n",
    "colnames = []\n",
    "\n",
    "for i in range(20):\n",
    "    colnames.append(i)\n",
    "\n",
    "df_test_label = pd.read_csv(file,sep=' ', names=colnames)\n",
    "df_test_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn dataframes into csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('train-data.csv')\n",
    "# df_test_final.to_csv('test-data.csv')\n",
    "# df_train_label.to_csv('train-label.csv')\n",
    "# df_test_label.to_csv('test-label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_centered = df_train.copy()\n",
    "df_train_centered = df_train_centered.sub(df_train_centered.mean(axis=1), axis=0)\n",
    "df_train_centered, df_train_centered.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copy the data\n",
    "df_train_normalized = df_train.copy()\n",
    "  \n",
    "# apply normalization techniques\n",
    "for column in df_train_normalized.columns:\n",
    "    df_train_normalized[column] = (df_train_normalized[column] - df_train_normalized[column].min()) / (df_train_normalized[column].max() - df_train_normalized[column].min())    \n",
    "  \n",
    "df_train_normalized['2287']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data\n",
    "df_train_standardized = df_train.copy()\n",
    "\n",
    "# apply standardization techniques\n",
    "df_train_standardized = (df_train_standardized-df_train_standardized.mean())/df_train_standardized.std()\n",
    "df_train_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_standardized.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.copy()\n",
    "scale_X = StandardScaler().fit(X)\n",
    "X_norm = scale_X.transform(X)\n",
    "X_norm\n",
    "df_standardized = pd.DataFrame(X_norm, columns = df_train.columns)\n",
    "df_standardized\n",
    "df_standardized.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.copy()\n",
    "scale_X = StandardScaler().fit(X)\n",
    "X_norm = scale_X.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized = pd.DataFrame(X_norm, columns = df_train.columns)\n",
    "df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_standardized.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
